{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have a dimension and a type. __hello_constant__ is a 0 dimensional string tensor.\n",
    "\n",
    "- tf.constant\n",
    "\n",
    "TensorFlow's api is built around the concept of a computational graph. \n",
    "\n",
    "TensorFlow Sessions is an enviroment for running a graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use TensorFlow __placeholder__ variables to feed values into the computational graph within a Session.\n",
    "\n",
    "- tf.placeholders requires a type. Can also provide a shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heyo\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "place_x = tf.placeholder(tf.string)\n",
    "with tf.Session() as session:\n",
    "    output = session.run(place_x,{place_x:\"Heyo\"})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing math in TensorFlow (full list of ops - https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops.html#math):\n",
    "\n",
    "- tf.add\n",
    "- tf.sub\n",
    "- tf.mul\n",
    "- tf.div "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int32)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.sub(tf.div(x,y),tf.constant(1))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    out = session.run(z,feed_dict={x:10,y:2})\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified.\n",
    "\n",
    "- This leaves out tf.placeholder() and tf.constant(), since those __Tensors can't be modified__. \n",
    "- This is where tf.Variable() comes in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(5)\n",
    "init = tf.initialize_all_variables() # <- returns an operation that we can call in a session to initialize all the variables\n",
    "y = tf.add(x,1)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    out = session.run(x)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a logistic classifier. \n",
    "\n",
    "- Given inputs x, we can use weights w and biases b to generate logits (scores) y. \n",
    "- x: N observations, K features\n",
    "- w: K features, L classes\n",
    "- b: 1, L classes\n",
    "\n",
    "We can calculate either:\n",
    "\n",
    "- w*x+b = y, or \n",
    "- x*w + b = y\n",
    "\n",
    "We will calculate x*w + b. \n",
    "\n",
    "Before computing this, and training our weights and biases, we should initialize our weights and biases. appropriate to use a random numbers drawn from a truncated standard normal to init our parameters. We use truncated normals so that:\n",
    "\n",
    "- Randomness allows for more variation when we restart the algorithm. Helps decrease the likelihood we fall into a local minimum\n",
    "- Small values prevent overfitting\n",
    "\n",
    "Since we want to update our weights/biases, we should represent them as TensorFlow __variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [0 1]]\n",
      "[[-1.68838227 -1.87039447 -1.45894945 -0.71141493 -0.059352  ]\n",
      " [-0.79901886 -0.23569219  0.10964096  0.03092351 -0.22617172]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "[[-3.28641987 -2.34177876 -1.23966753 -0.6495679  -0.51169544]\n",
      " [-0.79901886 -0.23569219  0.10964096  0.03092351 -0.22617172]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_features = 2\n",
    "n_classes = 5\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "weights = tf.Variable(tf.truncated_normal(shape=(n_features,n_classes)))\n",
    "biases = tf.Variable(tf.zeros(n_classes))\n",
    "\n",
    "logits = tf.add(tf.matmul(x,weights),biases)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "inp = np.array([[1,2],[0,1]])\n",
    "               \n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    print(inp)\n",
    "    print(session.run(weights))\n",
    "    print(session.run(biases))\n",
    "    out = session.run(logits,feed_dict={x:inp})\n",
    "    print(out)\n",
    "    \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert logits to probabilities using the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.33000561  0.33332222  0.33667217]\n",
      "[ 0.30060961  0.33222499  0.3671654 ]\n",
      "[ 0.09003057  0.24472847  0.66524096]\n",
      "[  2.06106005e-09   4.53978686e-05   9.99954600e-01]\n",
      "[  1.38389653e-87   3.72007598e-44   1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: array of logits\n",
    "    returns numpy array of same size with softmaxes\n",
    "    \"\"\"\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "\n",
    "print(softmax([.01,.02,.03]))\n",
    "print(softmax([.1,.2,.3]))\n",
    "print(softmax([1,2,3]))\n",
    "print(softmax([10,20,30]))\n",
    "print(softmax([100,200,300]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we multiple logits by 10, we see that the probabilities get closer to 0 or 1.\n",
    "- If we divide logits by 10, the probabilities become more uniform\n",
    "\n",
    "So, the magnitude of the logits is important. Initially, we want our logits to be small. As the model is trained and gets better, we want the magnitude of the logits to increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model. Cross Entropy, Average Cross Entropy, Optimizing avg Cross Entropy by updating weights/biases \n",
    "\n",
    "Now that we have softmaxes, we can calculate how closely our predictions are to the true label of our data. For this part, it helps to have our labels 1-hot encoded, or represented as vectors where all values are 0 except the index corresponding to the class. e.g. if there are 5 classes, and obs i is classified as 3, then its 1-hot encoding is [0,0,1,0,0].\n",
    "\n",
    "We can use the 1 hot encoding and softmaxes to calculate the cross-entropy of our model. Cross-entropy is a distance measure on 2 vectors defined as:\n",
    "\n",
    "d(S,L) = -1*sum(L\\_{i} * log(S\\_{i}))\n",
    "\n",
    "Where S are our predicted softmaxes and L is the one-hot encoded represented of the true label. Some key points:\n",
    "\n",
    "- Order matters for softmax, d(S,L) not necessarily equal to d(L,S)\n",
    "- Log is natural log\n",
    "- Our softmax functions will give a non-zero probability to every class, so the natural log will not be undefined.\n",
    "\n",
    "As in all machine learning problems, we do training by formulating a loss function and minimizing it. For this multinomial logistic regression, we can express our Loss function as a sum of d(S,L) over all the examples in our data, divided by the total number of examples in our data. \n",
    "\n",
    "Then we can minimize this loss function by taking it's gradient, setting it equal to zero, and incrementally updating our parameters in the direction of steepest descent for the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Input before Training\n",
    "\n",
    "- Bad conditioned vs. Well conditioned\n",
    "- numerical instability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9073486328125\n",
      "0.04632568359375\n",
      "0.0016222000122070312\n",
      "-0.00024044513702392578\n",
      "-7.614493370056152e-06\n",
      "6.9374218583106995e-06\n",
      "-3.3853575587272644e-07\n",
      "2.5247572921216488e-09\n",
      "2.5247572921216488e-09\n",
      "7.484004527213983e-10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for exp in range(10,0,-1):\n",
    "    a = 10**exp\n",
    "    b = a\n",
    "    for i in range(1000000):\n",
    "        a += 10**-6\n",
    "    print(1-(a-b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding really big and really small numbers gives unpredictable results. We see that as a,b -> 0, the error 1-(a-b) -> 0. We do not want numerical instability to impact the minimization of our loss function. So, for our input features, it is better to normalize the values to having 0 mean and equal variance before feeding them to the algorithm. For example, for images with 255 pixels, for each color channel, we can do:\n",
    "\n",
    "- r = (r+128)/128\n",
    "- g = (g+128)/128\n",
    "- b = (b+128)/128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Size\n",
    "\n",
    "If classes are well-balanced, can use rule of 30 as a proxy for determining validation set size:\n",
    "\n",
    "- Hold back more than 30000 examples for validation. \n",
    "- Accuracy changes >.1% are significant\n",
    "\n",
    "If classes are not well-balanced, as is the case for more real world examples, what can we do?\n",
    "\n",
    "- Resample to balance classes\n",
    "- generate synthetic data to rebalance\n",
    "- ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "- Gradient Descent runs into scalability issues for large data sets. The loss function over the entire dataset is computational intensive to calculate, and calculating gradients can be approx 3x more intensive (as a rule of thumb) than computing the loss function\n",
    "- So, we can approximate GD by training on small batches (< 1000 examples), and calculating the loss (average cross entropy) for these examples, assuming it is an approximation to the true loss (given the current parameters and data), and updating the parameters accordingly by propagating this error back through the network.\n",
    "- This is a scalable approach but not a great optimizer. Oftentimes, the gradient of these batches is not in the best direction and cost function may not be monotonically decreasing\n",
    "\n",
    "Tricks to implement SGD:\n",
    "    \n",
    "1. Inputs: 0 mean and equal variance\n",
    "2. Weights: random values, 0 mean, equal variance\n",
    "3. Momentum - running average of gradient to get the general direction in which we should update our parameters and move towards our objective.\n",
    "4. Learning Rate Decay - smaller noiser steps to objective. Beneficial to make the learning rate smaller as we train. Lower it over time is empirically shown to help.\n",
    "\n",
    "\n",
    "SGD Parameters:\n",
    "\n",
    "- initial weights/biases\n",
    "- initial learning rate\n",
    "- momentum\n",
    "- decay\n",
    "- batch size\n",
    "\n",
    "When things don't work, try lowering learning rate to start.\n",
    "\n",
    "#### Adagrad\n",
    "\n",
    "- implicitly, does momentum and learning rate decay\n",
    "- often makes learning less sensitive to hyper parameters\n",
    "- but, may be less performant than SGD with good tuning.\n",
    "- but, good place to start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CarND-TensorFlow-Lab]",
   "language": "python",
   "name": "conda-env-CarND-TensorFlow-Lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
