{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Basics\n",
    "\n",
    "Tensors have a dimension and a type. __hello_constant__ is a 0 dimensional string tensor.\n",
    "\n",
    "- tf.constant\n",
    "\n",
    "TensorFlow's api is built around the concept of a computational graph. \n",
    "\n",
    "TensorFlow Sessions is an enviroment for running a graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use TensorFlow __placeholder__ variables to feed values into the computational graph within a Session.\n",
    "\n",
    "- tf.placeholders requires a type. Can also provide a shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heyo\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "place_x = tf.placeholder(tf.string)\n",
    "with tf.Session() as session:\n",
    "    output = session.run(place_x,{place_x:\"Heyo\"})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing math in TensorFlow (full list of ops - https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops.html#math):\n",
    "\n",
    "- tf.add\n",
    "- tf.sub\n",
    "- tf.mul\n",
    "- tf.div "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int32)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.sub(tf.div(x,y),tf.constant(1))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    out = session.run(z,feed_dict={x:10,y:2})\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified.\n",
    "\n",
    "- This leaves out tf.placeholder() and tf.constant(), since those __Tensors can't be modified__. \n",
    "- This is where tf.Variable() comes in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(5)\n",
    "init = tf.initialize_all_variables() # <- returns an operation that we can call in a session to initialize all the variables\n",
    "x = tf.add(x,1)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    out = session.run(x)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a logistic classifier. \n",
    "\n",
    "- Given inputs x, we can use weights w and biases b to generate logits (scores) y. \n",
    "- x: N observations, K features\n",
    "- w: K features, L classes\n",
    "- b: 1, L classes\n",
    "\n",
    "We can calculate either:\n",
    "\n",
    "- w*x+b = y, or \n",
    "- x*w + b = y\n",
    "\n",
    "We will calculate x*w + b. \n",
    "\n",
    "Before computing this, and training our weights and biases, we should initialize our weights and biases. appropriate to use a random numbers drawn from a truncated standard normal to init our parameters. We use truncated normals so that:\n",
    "\n",
    "- Randomness allows for more variation when we restart the algorithm. Helps decrease the likelihood we fall into a local minimum\n",
    "- Small values prevent overfitting\n",
    "\n",
    "Since we want to update our weights/biases, we should represent them as TensorFlow __variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [0 1]]\n",
      "[[-1.07275569  0.58411843 -0.6492455   0.38904485 -0.93004668]\n",
      " [ 0.86994207 -1.79790843 -0.25243679  0.82723159  0.83809364]]\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "[[ 0.66712844 -3.01169848 -1.15411901  2.04350805  0.7461406 ]\n",
      " [ 0.86994207 -1.79790843 -0.25243679  0.82723159  0.83809364]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_features = 2\n",
    "n_classes = 5\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "weights = tf.Variable(tf.truncated_normal(shape=(n_features,n_classes)))\n",
    "biases = tf.Variable(tf.zeros(n_classes))\n",
    "\n",
    "logits = tf.add(tf.matmul(x,weights),biases)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "inp = np.array([[1,2],[0,1]])\n",
    "               \n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    print(inp)\n",
    "    print(session.run(weights))\n",
    "    print(session.run(biases))\n",
    "    out = session.run(logits,feed_dict={x:inp})\n",
    "    print(out)\n",
    "    \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert logits to probabilities using the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.33000561  0.33332222  0.33667217]\n",
      "[ 0.30060961  0.33222499  0.3671654 ]\n",
      "[ 0.09003057  0.24472847  0.66524096]\n",
      "[  2.06106005e-09   4.53978686e-05   9.99954600e-01]\n",
      "[  1.38389653e-87   3.72007598e-44   1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: array of logits\n",
    "    returns numpy array of same size with softmaxes\n",
    "    \"\"\"\n",
    "    return np.exp(x)/np.sum(np.exp(x),axis=0)\n",
    "\n",
    "print(softmax([.01,.02,.03]))\n",
    "print(softmax([.1,.2,.3]))\n",
    "print(softmax([1,2,3]))\n",
    "print(softmax([10,20,30]))\n",
    "print(softmax([100,200,300]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we multiple logits by 10, we see that the probabilities get closer to 0 or 1.\n",
    "- If we divide logits by 10, the probabilities become more uniform\n",
    "\n",
    "So, the magnitude of the logits is important. Initially, we want our logits to be small. As the model is trained and gets better, we want the magnitude of the logits to increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model. Cross Entropy, Average Cross Entropy, Optimizing avg Cross Entropy by updating weights/biases \n",
    "\n",
    "Now that we have softmaxes, we can calculate how closely our predictions are to the true label of our data. For this part, it helps to have our labels 1-hot encoded, or represented as vectors where all values are 0 except the index corresponding to the class. e.g. if there are 5 classes, and obs i is classified as 3, then its 1-hot encoding is [0,0,1,0,0].\n",
    "\n",
    "We can use the 1 hot encoding and softmaxes to calculate the cross-entropy of our model. Cross-entropy is a distance measure on 2 vectors defined as:\n",
    "\n",
    "d(S,L) = -1*sum(L\\_{i} * log(S\\_{i}))\n",
    "\n",
    "Where S are our predicted softmaxes and L is the one-hot encoded represented of the true label. Some key points:\n",
    "\n",
    "- Order matters for softmax, d(S,L) not necessarily equal to d(L,S)\n",
    "- Log is natural log\n",
    "- Our softmax functions will give a non-zero probability to every class, so the natural log will not be undefined.\n",
    "\n",
    "As in all machine learning problems, we do training by formulating a loss function and minimizing it. For this multinomial logistic regression, we can express our Loss function as a sum of d(S,L) over all the examples in our data, divided by the total number of examples in our data. \n",
    "\n",
    "Then we can minimize this loss function by taking it's gradient, setting it equal to zero, and incrementally updating our parameters in the direction of steepest descent for the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Input before Training\n",
    "\n",
    "- Bad conditioned vs. Well conditioned\n",
    "- numerical instability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9073486328125\n",
      "0.04632568359375\n",
      "0.0016222000122070312\n",
      "-0.00024044513702392578\n",
      "-7.614493370056152e-06\n",
      "6.9374218583106995e-06\n",
      "-3.3853575587272644e-07\n",
      "2.5247572921216488e-09\n",
      "2.5247572921216488e-09\n",
      "7.484004527213983e-10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for exp in range(10,0,-1):\n",
    "    a = 10**exp\n",
    "    b = a\n",
    "    for i in range(1000000):\n",
    "        a += 10**-6\n",
    "    print(1-(a-b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding really big and really small numbers gives unpredictable results. We see that as a,b -> 0, the error 1-(a-b) -> 0. We do not want numerical instability to impact the minimization of our loss function. So, for our input features, it is better to normalize the values to having 0 mean and equal variance before feeding them to the algorithm. For example, for images with 255 pixels, for each color channel, we can do:\n",
    "\n",
    "- r = (r+128)/128\n",
    "- g = (g+128)/128\n",
    "- b = (b+128)/128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Size\n",
    "\n",
    "If classes are well-balanced, can use rule of 30 as a proxy for determining validation set size:\n",
    "\n",
    "- Hold back more than 30000 examples for validation. \n",
    "- Accuracy changes >.1% are significant\n",
    "\n",
    "If classes are not well-balanced, as is the case for more real world examples, what can we do?\n",
    "\n",
    "- Resample to balance classes\n",
    "- generate synthetic data to rebalance\n",
    "- ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "- Gradient Descent runs into scalability issues for large data sets. The loss function over the entire dataset is computational intensive to calculate, and calculating gradients can be approx 3x more intensive (as a rule of thumb) than computing the loss function\n",
    "- So, we can approximate GD by training on small batches (< 1000 examples), and calculating the loss (average cross entropy) for these examples, assuming it is an approximation to the true loss (given the current parameters and data), and updating the parameters accordingly by propagating this error back through the network.\n",
    "- This is a scalable approach but not a great optimizer. Oftentimes, the gradient of these batches is not in the best direction and cost function may not be monotonically decreasing\n",
    "\n",
    "Tricks to implement SGD:\n",
    "    \n",
    "1. Inputs: 0 mean and equal variance\n",
    "2. Weights: random values, 0 mean, equal variance\n",
    "3. Momentum - running average of gradient to get the general direction in which we should update our parameters and move towards our objective.\n",
    "4. Learning Rate Decay - smaller noiser steps to objective. Beneficial to make the learning rate smaller as we train. Lower it over time is empirically shown to help.\n",
    "\n",
    "\n",
    "SGD Parameters:\n",
    "\n",
    "- initial weights/biases\n",
    "- initial learning rate\n",
    "- momentum\n",
    "- decay\n",
    "- batch size\n",
    "\n",
    "When things don't work, try lowering learning rate to start.\n",
    "\n",
    "#### Adagrad\n",
    "\n",
    "- implicitly, does momentum and learning rate decay\n",
    "- often makes learning less sensitive to hyper parameters\n",
    "- but, may be less performant than SGD with good tuning.\n",
    "- but, good place to start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Deep NN Flow\n",
    "\n",
    "- Read in data and preprocess\n",
    "    - Normalization\n",
    "    - 1 hot encoding labels\n",
    "    - split into train, validation, test\n",
    "\n",
    "- Define Learning Parameters\n",
    "    - learning rate\n",
    "    - training epochs\n",
    "    - batch size\n",
    "\n",
    "- Define Input Parameters\n",
    "    - Features Size\n",
    "    - Number of Classes\n",
    "\n",
    "- Define the number of units in each hidden layer\n",
    "- Initialize weights and biases for each layer\n",
    "- Input\n",
    "    - Placeholder variables for x, labels\n",
    "    - reshape x if necessary\n",
    "\n",
    "- Multilayer Perceptron\n",
    "    - layer 1 output\n",
    "    - layer 2 output \n",
    "    - etc\n",
    "    \n",
    "- Optimizer\n",
    "    - Use softmax to convert logits to scores\n",
    "    - define average cross entropy as a function of the true labels (1 hot encoded) and logits\n",
    "    - Choose SGD or ADAGRAD as gradient descent implementation to use to minimize loss function\n",
    "    \n",
    "- Session\n",
    "    - initialize and run variables\n",
    "    - for each training epoch, get a batch of __batch_size__, run optimizer with learning rate __learning_rate__ and feed in current batch, current labels. \n",
    "    - Calculate validation error after each epoch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "0.1135\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "\n",
    "layers = {\n",
    "    'layer_1':256,\n",
    "    'layer_2':512\n",
    "}\n",
    "\n",
    "weights = {\n",
    "    'layer_1':tf.Variable(tf.truncated_normal([n_input,layers['layer_1']])),\n",
    "    'layer_2':tf.Variable(tf.truncated_normal([layers['layer_1'],layers['layer_2']])),\n",
    "    'output':tf.Variable(tf.truncated_normal([layers['layer_2'],n_classes]))\n",
    "\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'layer_1': tf.Variable(tf.truncated_normal([layers['layer_1']])),\n",
    "    'layer_2': tf.Variable(tf.truncated_normal([layers['layer_2']])),\n",
    "    'output': tf.Variable(tf.truncated_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,28,28,1])\n",
    "y = tf.placeholder(dtype=tf.float32,shape=[None,n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x,[-1,n_input])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "def multi_layer_perceptron(x_flat,weights,biases,keep_prob):\n",
    "    \n",
    "    layer_1 = tf.add(tf.matmul(x_flat,weights['layer_1']),biases['layer_1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1,keep_prob)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(layer_1,weights['layer_2']),biases['layer_2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_2 = tf.nn.dropout(layer_2,keep_prob=keep_prob)\n",
    "    \n",
    "    logits = tf.add(tf.matmul(layer_2,weights['output']),biases['output'])\n",
    "    return(logits)\n",
    "    \n",
    "logits = multi_layer_perceptron(x_flat,weights,biases,keep_prob)\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits,y)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits,1),tf.argmax(y,1)),tf.float32))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x,batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            session.run(optimizer,feed_dict={x: batch_x,\n",
    "                                            y: batch_y,\n",
    "                                            keep_prob:.5})\n",
    "        print(epoch)\n",
    "    \n",
    "        \n",
    "    test_x,test_y = mnist.test.next_batch(batch_size=10000)\n",
    "    print(session.run(accuracy,feed_dict={x:test_x,\n",
    "                                 y:test_y,\n",
    "                                 keep_prob:1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to Prevent Overfitting\n",
    "\n",
    "- Early Termination: Stop training when validation performance begins to fall\n",
    "- L2 Regularization: Add penalty term to loss function, that is the squared sum of the weights\n",
    "- Dropout: Force the network to hold redundant representations of information. Dropout randomly sets the activations for x% of nodes in a hidden layer to 0. So, for the same input, it's possible that we get different nodes set to 0, that could lead to different predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "- NN's that share their parameters across space\n",
    "- width, height, depth\n",
    "\n",
    "#### Weight Sharing\n",
    "\n",
    "- Enforces invariance\n",
    "- reduces parameter space of problem\n",
    "- squeezes higher order features out of image\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- number of filters\n",
    "- a convolution is the application of a filter across an image\n",
    "- the result of a convolution is a feature map\n",
    "- patch/kernel: height, width, depth\n",
    "- stride: how many pixels do we shift our filter each time we move it?\n",
    "- padding: how to handle the edges? 'valid' or 'same' (adds a padding of zero's to the outside of the image so that the output map size is the same as the input map)\n",
    "\n",
    "#### Calculate volume of output\n",
    "\n",
    "Given our input layer has a volume of W, our filter has a volume (height * width * depth) of F, we have a stride of S, and a padding of P, the following formula gives us the volume of the next layer: (W−F+2P)/S+1.\n",
    "\n",
    "#### Calculate number of parameters (with and without weight sharing)\n",
    "\n",
    "Input Image: 32x32x5\n",
    "Filters: Apply 20 filters of 8x8x3\n",
    "Stride: 2\n",
    "Padding: 1\n",
    "\n",
    "1. new height: (32-8+2x1)/2 + 1 = 14\n",
    "2. new width: (32-8+2x1)/2 + 1 = 14\n",
    "3. new depth: # of filters = 20\n",
    "\n",
    "##### Number of Parameters without weight sharing\n",
    "\n",
    "- Connection between every nueron in filter and every neuron in output map, plus bias neurons\n",
    "- (8x8x3 + 1)x 14x14x20 = 756560\n",
    "\n",
    "##### Number of Parameters with weight sharing\n",
    "\n",
    "- Connection between every nueron in filter and each output map, plus bias neurons\n",
    "- (8x8x3 + 1)x 20 = 3860\n",
    "\n",
    "We use 196X fewer parameters when using weight sharing!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) # (height, width, input_depth, output_depth)\n",
    "filter_bias = tf.Variable(tf.zeros(20))\n",
    "strides = [1, 2, 2, 1] # (batch, height, width, depth)\n",
    "padding = 'VALID'\n",
    "conv_layer = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements to Simple ConvNets\n",
    "\n",
    "- Pooling\n",
    "- 1x1 Convolutions\n",
    "- Inception Architecture\n",
    "\n",
    "#### Pooling\n",
    "\n",
    "- at every point on feature map, look at a small neighborhood around it and compute maximum\n",
    "- doesnt add to parameters\n",
    "- makes computation more expensive, because we have presumably modified the convolution in the previous layer to have a lower stride.\n",
    "- adds hyper parameters (pooling region size, pooling stride)\n",
    "- le net 5 (first to use max pooling), alex net. \n",
    "- instead of taking the max, can take the average\n",
    "\n",
    "- Pooling decreases the size of the output and prevent overfitting. Preventing overfitting is a consequence of the reducing the output size, which in turn, reduces the number of parameters in future layers.\n",
    "\n",
    "Recent datasets are so big and complex we're more concerned about underfitting.\n",
    "Dropout is a much better regularizer.\n",
    "Pooling results in a loss of information. Think about the max pooling operation as an example. We only keep the largest of n numbers, thereby disregarding n-1 numbers completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply Max Pooling\n",
    "conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 9.523663521\n",
      "Epoch: 0002 cost= 6.950458050\n",
      "Epoch: 0003 cost= 6.193962097\n",
      "Epoch: 0004 cost= 5.135995865\n",
      "Epoch: 0005 cost= 4.015914917\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8fbfba6f2849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Run optimization op (backprop) and cost op (to get loss value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Display logs per epoch step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/CarND-TensorFlow-Lab/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "training_epochs = 30\n",
    "\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "layer_depth = {\n",
    "    'layer_1': 32,\n",
    "    'layer_2': 64,\n",
    "    'layer_3': 128,\n",
    "    'fully_connected': 512\n",
    "}\n",
    "\n",
    "weights = {\n",
    "    'layer_1': tf.Variable(tf.truncated_normal(\n",
    "        [5, 5, 1, layer_depth['layer_1']])),\n",
    "    'layer_2': tf.Variable(tf.truncated_normal(\n",
    "        [5, 5, layer_depth['layer_1'], layer_depth['layer_2']])),\n",
    "    'layer_3': tf.Variable(tf.truncated_normal(\n",
    "        [5, 5, layer_depth['layer_2'], layer_depth['layer_3']])),\n",
    "    'fully_connected': tf.Variable(tf.truncated_normal(\n",
    "        [4*4*128, layer_depth['fully_connected']])),\n",
    "    'out': tf.Variable(tf.truncated_normal(\n",
    "        [layer_depth['fully_connected'], n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'layer_1': tf.Variable(tf.zeros(layer_depth['layer_1'])),\n",
    "    'layer_2': tf.Variable(tf.zeros(layer_depth['layer_2'])),\n",
    "    'layer_3': tf.Variable(tf.zeros(layer_depth['layer_3'])),\n",
    "    'fully_connected': tf.Variable(tf.zeros(layer_depth['fully_connected'])),\n",
    "    'out': tf.Variable(tf.zeros(n_classes))\n",
    "}\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, k, k, 1],\n",
    "        strides=[1, k, k, 1],\n",
    "        padding='SAME')\n",
    "\n",
    "\n",
    "def conv_net(x, weights, biases):\n",
    "    # Layer 1 - 28*28*1 to 14*14*32\n",
    "    conv1 = conv2d(x, weights['layer_1'], biases['layer_1'])\n",
    "    conv1 = maxpool2d(conv1)\n",
    "\n",
    "    # Layer 2 - 14*14*32 to 7*7*64\n",
    "    conv2 = conv2d(conv1, weights['layer_2'], biases['layer_2'])\n",
    "    conv2 = maxpool2d(conv2)\n",
    "\n",
    "    # Layer 3 - 7*7*64 to 4*4*128\n",
    "    conv3 = conv2d(conv2, weights['layer_3'], biases['layer_3'])\n",
    "    conv3 = maxpool2d(conv3)\n",
    "\n",
    "    # Fully connected layer - 4*4*128 to 512\n",
    "    # Reshape conv3 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(\n",
    "        conv3,\n",
    "        [-1, weights['fully_connected'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(\n",
    "        tf.matmul(fc1, weights['fully_connected']),\n",
    "        biases['fully_connected'])\n",
    "    fc1 = tf.nn.tanh(fc1)\n",
    "\n",
    "    # Output Layer - class prediction - 512 to 10\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "logits = conv_net(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        # Display logs per epoch step\n",
    "        c = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CarND-TensorFlow-Lab]",
   "language": "python",
   "name": "conda-env-CarND-TensorFlow-Lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
